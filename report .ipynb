{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ddb918",
   "metadata": {},
   "source": [
    "Student Information\n",
    "Name:蕭宜芳\n",
    "Student ID:112034584\n",
    "GitHub ID:yy223xiao\n",
    "Kaggle name:Xiaoooyifang\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24830bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "height": 400,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"C://Users//yuan//Desktop//DM2024-Lab2-Master-main//pic0.PNG\",width=400,height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc624d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5033f6",
   "metadata": {},
   "source": [
    "1.Read Data:Load the tweet data from a JSON file. Each tweet is extracted and stored in a list, which is then converted into a DataFrame for further processing.\n",
    "2.Load Data Identification:Import the data identification file, which contains metadata or unique identifiers for each tweet.\n",
    "3.Merge the Emotion Labels and Data Identification Information:Combine the emotion labels and identification data with the main tweet dataset using the common tweet_id column. This ensures all necessary information is unified in one DataFrame.\n",
    "4.Filter Rows That Have an Emotion Label:Remove rows that lack emotion labels (NaN) to focus on tweets with defined emotion categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets_data = []\n",
    "with open('tweets_DM.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet[\"_source\"][\"tweet\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "tweets_df = pd.DataFrame(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a628b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotion labels\n",
    "emotion_df = pd.read_csv('emotion.csv')\n",
    "\n",
    "# Load data identification\n",
    "identification_df = pd.read_csv('data_identification.csv')\n",
    "\n",
    "# Merge the emotion labels and data identification information\n",
    "tweets_df = tweets_df.merge(emotion_df, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "tweets_df = tweets_df.merge(identification_df, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "\n",
    "# Filter rows that have an emotion label\n",
    "labeled_tweets_df = tweets_df[~tweets_df['emotion'].isna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6c5bf",
   "metadata": {},
   "source": [
    "Reasons for Choosing the LightGBM Model:\n",
    "1.Efficiency and Scalability:LightGBM is highly efficient, making it suitable for handling large, high-dimensional datasets like TF-IDF features from text.\n",
    "2.Support for Multiclass Classification:LightGBM natively supports multiclass classification, essential for emotion recognition tasks.\n",
    "3.Handling Sparse Data:It excels with sparse, high-dimensional data, which aligns well with text data represented as TF-IDF.\n",
    "4.Regularization:Built-in regularization techniques (e.g., lambda_l1, lambda_l2) help reduce overfitting, crucial for text data.\n",
    "5.Fast Training:Its histogram-based algorithm ensures faster training and prediction, allowing for efficient experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763e6f0",
   "metadata": {},
   "source": [
    "1.Dataset Preparation:The dataset is split into training and testing subsets based on the identification column.Training data contains rows with identification == 'train'.Testing data contains rows with identification == 'test'.\n",
    "2.Handling Missing Values:Ensure all text inputs are strings and handle any missing values by replacing None or NaN with an empty string.\n",
    "3.Train-Validation Split:The training dataset is split into a training set and a validation set to evaluate model performance during training.The split is stratified to maintain the proportion of each emotion category.\n",
    "4.Label Encoding;Encode categorical emotion labels into numerical format using LabelEncoder.\n",
    "5.TF-IDF Vectorization:Convert text data into numerical feature representations using TF-IDF (Term Frequency-Inverse Document Frequency).The vectorizer is configured with:max_features=10000: Use up to 10,000 most relevant features.ngram_range=(1, 2): Include both unigrams and bigrams.The vectorizer is applied to the training, validation, and test datasets.\n",
    "6.Dataset Creation for LightGBM:Prepare datasets in the format required by LightGBM for training and validation.\n",
    "7.Model Configuration:\n",
    "    Set the parameters for the LightGBM model:\n",
    "    objective: Multiclass classification.\n",
    "    num_class: Number of emotion categories (from LabelEncoder).\n",
    "    boosting_type: Gradient Boosted Decision Trees (gbdt).\n",
    "    metric: Multi-class log loss (multi_logloss).\n",
    "    num_leaves: Maximum number of leaves per tree (128).\n",
    "    learning_rate: Learning rate (0.03).\n",
    "    max_bin: Maximum number of bins for discretizing continuous features (255).\n",
    "    feature_fraction: Use 70% of features for training.\n",
    "    bagging_fraction: Use 60% of samples for training.\n",
    "    bagging_freq: Perform bagging every 5 iterations.\n",
    "    min_data_in_leaf: Minimum data points in each leaf (100) to avoid overfitting.\n",
    "    lambda_l1 and lambda_l2: Regularization parameters to reduce model complexity.\n",
    "    seed: Random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f4b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "train_data = tweets_df[tweets_df['identification'] == 'train']\n",
    "test_data = tweets_df[tweets_df['identification'] == 'test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f087956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_text = train_data['text'].values\n",
    "y_train_text = train_data['emotion'].values\n",
    "X_test_text = test_data['text'].values\n",
    "test_ids = test_data['tweet_id'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8992616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train_text = np.array([\"\" if x is None else str(x) for x in X_train_text])  # Handle None/NaN and ensure strings\n",
    "X_test_text = np.array([\"\" if x is None else str(x) for x in X_test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec924d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_val_text, y_train_text, y_val_text = train_test_split(\n",
    "    X_train_text, y_train_text, test_size=0.2, random_state=42, stratify=y_train_text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1554c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_text)\n",
    "y_val = le.transform(y_val_text)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_val_tfidf = vectorizer.transform(X_val_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfef6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lgb.Dataset(X_train_tfidf, label=y_train)\n",
    "val_dataset = lgb.Dataset(X_val_tfidf, label=y_val, reference=train_dataset)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": len(le.classes_),\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"num_leaves\": 128,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"max_bin\": 255,\n",
    "    \"feature_fraction\": 0.7,  # 使用 70% 特徵\n",
    "    \"bagging_fraction\": 0.6,  # 使用 60% 樣本\n",
    "    \"bagging_freq\": 5,\n",
    "    \"min_data_in_leaf\": 100,  # 增加最小葉子數據量\n",
    "    \"lambda_l1\": 2.0,         # 強化正則化\n",
    "    \"lambda_l2\": 2.0,\n",
    "    \"seed\": 42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9bc13",
   "metadata": {},
   "source": [
    "Model Training\n",
    "1.The lgb.train() function is used to train the LightGBM model:Parameters (params): Includes settings for multiclass classification and regularization.Training Data (train_dataset): The dataset used to fit the model.Validation Data (val_dataset): Used to monitor performance and avoid overfitting.\n",
    "2.Predictions are made on the test data (X_test_tfidf) using the trained model:model.predict(): Generates probability predictions for each class.argmax(axis=1): Converts probabilities to class labels by selecting the class with the highest probability.\n",
    "3.Inverse Transform of Labels:Convert predicted numeric class labels back to their original emotion labels using the LabelEncoder.\n",
    "4.Result Formatting:Combine test IDs with predicted emotions into a list of dictionaries for easier interpretation and saving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224926b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 11.943399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 542415\n",
      "[LightGBM] [Info] Number of data points in the train set: 1164450, number of used features: 9997\n",
      "[LightGBM] [Info] Start training from score -3.597586\n",
      "[LightGBM] [Info] Start training from score -1.765956\n",
      "[LightGBM] [Info] Start training from score -2.347946\n",
      "[LightGBM] [Info] Start training from score -3.124284\n",
      "[LightGBM] [Info] Start training from score -1.037009\n",
      "[LightGBM] [Info] Start training from score -2.018193\n",
      "[LightGBM] [Info] Start training from score -3.396878\n",
      "[LightGBM] [Info] Start training from score -1.957811\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's multi_logloss: 1.35922\tvalid_1's multi_logloss: 1.36822\n",
      "[100]\ttraining's multi_logloss: 1.27209\tvalid_1's multi_logloss: 1.28754\n",
      "[150]\ttraining's multi_logloss: 1.22996\tvalid_1's multi_logloss: 1.25174\n",
      "[200]\ttraining's multi_logloss: 1.20395\tvalid_1's multi_logloss: 1.23185\n",
      "[250]\ttraining's multi_logloss: 1.18503\tvalid_1's multi_logloss: 1.21875\n",
      "[300]\ttraining's multi_logloss: 1.17009\tvalid_1's multi_logloss: 1.20954\n",
      "[350]\ttraining's multi_logloss: 1.1575\tvalid_1's multi_logloss: 1.20235\n",
      "[400]\ttraining's multi_logloss: 1.14664\tvalid_1's multi_logloss: 1.19685\n",
      "[450]\ttraining's multi_logloss: 1.13683\tvalid_1's multi_logloss: 1.1922\n",
      "[500]\ttraining's multi_logloss: 1.12799\tvalid_1's multi_logloss: 1.18839\n",
      "[550]\ttraining's multi_logloss: 1.11992\tvalid_1's multi_logloss: 1.1853\n",
      "[600]\ttraining's multi_logloss: 1.11246\tvalid_1's multi_logloss: 1.18263\n",
      "[650]\ttraining's multi_logloss: 1.10525\tvalid_1's multi_logloss: 1.18021\n",
      "[700]\ttraining's multi_logloss: 1.09857\tvalid_1's multi_logloss: 1.17826\n",
      "[750]\ttraining's multi_logloss: 1.09221\tvalid_1's multi_logloss: 1.17664\n",
      "[800]\ttraining's multi_logloss: 1.08618\tvalid_1's multi_logloss: 1.17527\n",
      "[850]\ttraining's multi_logloss: 1.08021\tvalid_1's multi_logloss: 1.174\n",
      "[900]\ttraining's multi_logloss: 1.07461\tvalid_1's multi_logloss: 1.17292\n",
      "[950]\ttraining's multi_logloss: 1.06901\tvalid_1's multi_logloss: 1.17183\n",
      "[1000]\ttraining's multi_logloss: 1.06377\tvalid_1's multi_logloss: 1.17098\n",
      "[1050]\ttraining's multi_logloss: 1.05858\tvalid_1's multi_logloss: 1.1703\n",
      "[1100]\ttraining's multi_logloss: 1.0535\tvalid_1's multi_logloss: 1.16959\n",
      "[1150]\ttraining's multi_logloss: 1.04853\tvalid_1's multi_logloss: 1.16895\n",
      "[1200]\ttraining's multi_logloss: 1.04374\tvalid_1's multi_logloss: 1.16844\n",
      "[1250]\ttraining's multi_logloss: 1.03911\tvalid_1's multi_logloss: 1.168\n",
      "[1300]\ttraining's multi_logloss: 1.03461\tvalid_1's multi_logloss: 1.16767\n",
      "[1350]\ttraining's multi_logloss: 1.03012\tvalid_1's multi_logloss: 1.16737\n",
      "[1400]\ttraining's multi_logloss: 1.02574\tvalid_1's multi_logloss: 1.16707\n",
      "[1450]\ttraining's multi_logloss: 1.02137\tvalid_1's multi_logloss: 1.16674\n",
      "[1500]\ttraining's multi_logloss: 1.01716\tvalid_1's multi_logloss: 1.16649\n",
      "[1550]\ttraining's multi_logloss: 1.01302\tvalid_1's multi_logloss: 1.16635\n",
      "[1600]\ttraining's multi_logloss: 1.0089\tvalid_1's multi_logloss: 1.16618\n",
      "[1650]\ttraining's multi_logloss: 1.00485\tvalid_1's multi_logloss: 1.16601\n",
      "[1700]\ttraining's multi_logloss: 1.00087\tvalid_1's multi_logloss: 1.16588\n",
      "[1750]\ttraining's multi_logloss: 0.996916\tvalid_1's multi_logloss: 1.16576\n",
      "[1800]\ttraining's multi_logloss: 0.99304\tvalid_1's multi_logloss: 1.16566\n",
      "[1850]\ttraining's multi_logloss: 0.989224\tvalid_1's multi_logloss: 1.16568\n",
      "[1900]\ttraining's multi_logloss: 0.985443\tvalid_1's multi_logloss: 1.16574\n",
      "Early stopping, best iteration is:\n",
      "[1837]\ttraining's multi_logloss: 0.990204\tvalid_1's multi_logloss: 1.16562\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    params,\n",
    "    train_dataset,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_dataset, val_dataset],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=50)  # 減少日誌顯示頻率\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35fd8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b337c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測結果已保存到 predicted_emotionsv2.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predicted_classes = predictions_proba.argmax(axis=1)\n",
    "predicted_emotions = le.inverse_transform(predicted_classes)\n",
    "results = [{\"id\": id_, \"emotion\": emotion} for id_, emotion in zip(test_ids, predicted_emotions)]\n",
    "\n",
    "# 將結果保存為 JSON 文件\n",
    "output_file = \"predicted_emotionsv2.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"預測結果已保存到 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431b2e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 已成功轉換為 CSV，保存至 predicted_emotionsv2.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = \"predicted_emotionsv2.json\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 將 JSON 數據轉換為 DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 保存為 CSV 文件\n",
    "output_file = \"predicted_emotionsv2.csv\"\n",
    "df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"JSON 已成功轉換為 CSV，保存至 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe7abc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 文件共有 411972 筆資料。\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('predicted_emotions.csv')\n",
    "num_records = len(df)\n",
    "print(f\"CSV 文件共有 {num_records} 筆資料。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12abcf",
   "metadata": {},
   "source": [
    "Using common metrics like accuracy, precision, recall, and F1-score. This code assumes you have y_val (true labels) and predictions_proba (predicted probabilities) for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dfc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Convert predicted probabilities to predicted classes for the validation set\n",
    "val_predicted_classes = model.predict(X_val_tfidf).argmax(axis=1)\n",
    "val_predicted_emotions = le.inverse_transform(val_predicted_classes)\n",
    "true_emotions = le.inverse_transform(y_val)\n",
    "\n",
    "# Evaluate metrics\n",
    "accuracy = accuracy_score(true_emotions, val_predicted_emotions)\n",
    "precision = precision_score(true_emotions, val_predicted_emotions, average='weighted')\n",
    "recall = recall_score(true_emotions, val_predicted_emotions, average='weighted')\n",
    "f1 = f1_score(true_emotions, val_predicted_emotions, average='weighted')\n",
    "\n",
    "# Detailed classification report\n",
    "report = classification_report(true_emotions, val_predicted_emotions)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Save evaluation results to a file\n",
    "evaluation_results = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1,\n",
    "    \"classification_report\": report\n",
    "}\n",
    "\n",
    "output_evaluation_file = \"evaluation_results.json\"\n",
    "with open(output_evaluation_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Evaluation results saved to {output_evaluation_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other model :predicted_emotions_lstm5555.csv\n",
    "#the score is 0.28369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d20d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "train_data = labeled_tweets_df\n",
    "test_data = tweets_df['identification'] == 'test'\n",
    "X_train_data = train_data['text'].values\n",
    "y_train_data = train_data['emotion'].values\n",
    "\n",
    "# 編碼情緒標籤\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_data)\n",
    "\n",
    "# 訓練與驗證集分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "# 文字 Tokenization 與 Padding\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "###Bidirectional LSTM ##################################\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPooling1D, Dropout, Dense\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(len(word_index) + 1, 10000)\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# 構建詞嵌入矩陣（使用 tqdm 查看進度）\n",
    "for word, i in tqdm(word_index.items(), desc=\"Building Embedding Matrix\"):\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "        \n",
    "# 替換 CNN 為 Bidirectional LSTM 模型\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),  # 雙向 LSTM 層\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(le.classes_), activation='softmax')  # 多分類輸出\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',  # 使用 Adam 优化器\n",
    "    loss='sparse_categorical_crossentropy',  # 使用稀疏交叉熵损失函数\n",
    "    metrics=['accuracy']  # 指标包括准确率\n",
    ")\n",
    "# 使用 GPU 訓練\n",
    "with tf.device('/gpu:0'):  # 強制使用 GPU\n",
    "    history = model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        batch_size=128,  # 批量大小\n",
    "        epochs=10,       # 訓練 epoch 數\n",
    "        verbose=1        # 顯示訓練日誌\n",
    "    )\n",
    "###Bidirectional LSTM ##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other model :emotion_predictions_CNN.csv\n",
    "#the score is 0.0.27182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code\n",
    "# 初始化詞嵌入矩陣\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(len(word_index) + 1, 10000)\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tqdm(word_index.items(), desc=\"Building Embedding Matrix\"):\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "# 構建 CNN 模型\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
    "    Conv1D(filters=256, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):  \n",
    "    history = model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c66c24",
   "metadata": {},
   "source": [
    "Model Feature Comparison\n",
    "LightGBM (Gradient Boosting Tree Model)\n",
    "Characteristics:\n",
    "\n",
    "Strong capability to handle sparse and high-dimensional features (e.g., TF-IDF or Bag-of-Words representations).\n",
    "Quick convergence with minimal tuning, suitable for small sample datasets.\n",
    "Fast training speed, particularly efficient on small to medium-sized datasets.\n",
    "Advantages:\n",
    "Excels at structured data and handcrafted features.\n",
    "Robust against noise and nonlinear relationships in data.\n",
    "Efficient in handling imbalanced datasets.\n",
    "LSTM (Long Short-Term Memory Network)\n",
    "Characteristics:\n",
    "\n",
    "Specially designed for sequential data, capable of capturing long-term dependencies.\n",
    "Suitable for processing longer sentences to extract contextual semantics.\n",
    "Disadvantages:\n",
    "Longer training time and heavily reliant on large datasets for good performance.\n",
    "Prone to overfitting, especially on small datasets, leading to unstable results.\n",
    "CNN (Convolutional Neural Network)\n",
    "Characteristics:\n",
    "\n",
    "Excels at capturing local contextual features (e.g., keyword sequences).\n",
    "Faster training compared to LSTM and less reliant on long-term dependencies.\n",
    "Disadvantages:\n",
    "Limited in capturing global semantic information across sequences.\n",
    "May underperform tree models when feature engineering is insufficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
